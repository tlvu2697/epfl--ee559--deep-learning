{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. DAG networks, autograd, convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Initialization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DAG Networks\n",
    "- **Writing from scratch a large neural network is complex and error-prone**\n",
    "- PyTorch, Caffe2, TensorFlow, MXNet, CNTK, Torch, Theano, Caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Autograd\n",
    "- Automatically constructed gradient\n",
    "- **The specification of the graph (DAG) looks a lot like the forward pass, and the operations of the forward pass define the backward pass**\n",
    "- Benefit of augograd:\n",
    "    1. Simpler syntax: just need forward pass, backward pass will automatically be constructed\n",
    "    2. Greater flexibility: since the graph is not static, forward pass can be dinamically modulated\n",
    "- To use autograd, use `torch.autograd.Variable` instead of `torch.Tensor`\n",
    "- `Variable`\n",
    "> - `data` : `Tensor`\n",
    "> - `grad` : `Variable`\n",
    "> - `requires_grad` : `Boolean`\n",
    "- `Parameter` is a `Variable` with `requires_grad` to `True`\n",
    "- Usage:\n",
    "    1. `torch.autograd.grad(outputs, inputs)`\n",
    "        - To generate the computational graph for computing **higher-order derivatives**: passing `create_graph=True`\n",
    "    2. `torch.autograd.backward(variables)` or `Variable.backward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Example: (Partial Derivative)**\n",
    "    - $(x_{1}, x_{2}, x_{3}) = (1, 2, 2)$\n",
    "    - $l = norm(x) = || x || = \\sqrt[]{x_{1}^{2} + x_{2}^{2} + x_{3}^{2}} = 3$  \n",
    "    $\\rightarrow \\frac{\\delta l}{\\delta x_{i}} = \\frac{x_{i}}{||x||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "(tensor([ 0.3333,  0.6667,  0.6667]),)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Example of Autograd using `torch.autograd.grad()`\n",
    "\"\"\"\n",
    "x = Variable(Tensor([1, 2, 2]), requires_grad = True)\n",
    "l = x.norm()\n",
    "print(l)\n",
    "\n",
    "g = torch.autograd.grad(l, x)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor([ 0.3333,  0.6667,  0.6667])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Example of Autograd using `torch.autograd.backward()`\n",
    "\"\"\"\n",
    "x = Variable(Tensor([1, 2, 2]), requires_grad = True)\n",
    "l = x.norm()\n",
    "print(l)\n",
    "\n",
    "l.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Example: run forward/backward pass**\n",
    "<img width=60% src=\"images/4a-1.png\">\n",
    "Architecture:  \n",
    "    - $\\phi^{(1)}(x^{(0)}; w^{(1)}) = w^{(1)}x^{(0)}$  \n",
    "    - $\\phi^{(2)}(x^{(0)}, x^{(1)}; w^{(2)}) = x^{(0)} + w^{(2)}x^{(1)}$  \n",
    "    - $\\phi^{(3)}(x^{(1)}, x^{(2)}; w^{(1)}) = w^{(1)}(x^{(1)} + x^{(2)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.6648)\n",
      "tensor([ 2.8595,  5.9130,  5.7412, -7.1607,  2.9478])\n",
      "tensor([ 0.2451,  0.5069,  0.4922, -0.6139,  0.2527])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Example of running forward/backward pass\n",
    "    Problem: Tensor.mv -> Tensor (not Varible/Parameter)\n",
    "\"\"\"\n",
    "w1 = Parameter(Tensor(5, 5).normal_())\n",
    "w2 = Parameter(Tensor(5, 5).normal_())\n",
    "x = Variable(Tensor(5).normal_(), requires_grad = True)\n",
    "\n",
    "x0 = x\n",
    "x1 = Variable(w1.mv(x0), requires_grad = True)\n",
    "x2 = x0 + Variable(w2.mv(x1), requires_grad = True)\n",
    "x3 = Variable(w1.mv(x1 + x2), requires_grad = True)\n",
    "\n",
    "q = x3.norm()\n",
    "q.backward()\n",
    "\n",
    "print(q)\n",
    "print(x3)\n",
    "print(x3.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Weight sharing\n",
    "- In the example above, both $\\phi^{(1)}$ and $\\phi^{(3)}$ use the same weight $w^{(1)}$. That's called **weight sharing**\n",
    "- Allow building **siamese networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convolutional layers (Stride = 1)\n",
    "- **A representation meaningful at a certain location can/should be used everywhere**\n",
    "- Main idea:\n",
    "<img width=60% src=\"images/4a-2.png\">\n",
    "- Usages:\n",
    "    1. Differential operator:\n",
    "        <img width=60% src=\"images/4a-3.png\">\n",
    "    2. Template matcher:\n",
    "        <img width=60% src=\"images/4a-4.png\">\n",
    "- Higher-dimension: \n",
    "> - `C` : channel\n",
    "---\n",
    "<img width=60% src=\"images/4a-5.png\">  \n",
    "<img width=60% src=\"images/4a-6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pooling (Down-scaling, Stride = Kernel_Size)\n",
    "- `max-pooling`: compute max values per block\n",
    "    <img width=60% src=\"images/4a-7.png\">\n",
    "- `average-pooling`: compute average values per block\n",
    "- Higher-dimension: \n",
    "> - `C` : channel\n",
    "---\n",
    "<img width=60% src=\"images/4a-8.png\">  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
