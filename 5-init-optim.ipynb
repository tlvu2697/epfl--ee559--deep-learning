{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Losses, optimization, and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Initialization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import os.path\n",
    "from torch import cuda, nn, optim, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cross-entropy\n",
    "- Mean-Squared Error is not the best loss function for Classification due to **conceptually wrong** $\\rightarrow$ Cross-entropy (kinda better choice)\n",
    "- Given 2 distributions $p$ and $q$, their cross-entropy is defined as:\n",
    "    $$\\text{H}(p, q) = - \\sum^{}_{k} p(k)\\,\\text{log}\\,q(k)$$\n",
    "- `torch.nn.CrossEntropyLoss` :\n",
    "    $$L(w) = -\\frac{1}{N}\\sum^{N}_{n=1}\\text{log}(\\frac{\\text{exp }f_{y_{n}(x_{n}; w)}}{\\sum_{k}\\text{exp }f_{k}(x_{n}; w)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5141)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Example of 'torch.nn.CrossEntropy.Loss'\n",
    "\"\"\"\n",
    "\n",
    "f = Variable(Tensor([[-1, -3, 4], [-3, 3, -1]]))\n",
    "target = Variable(torch.LongTensor([0, 1]))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(criterion(f, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=50% src=\"images/5-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In two-class problem with $x$ axis is the activation of the correct output unit, and the $y$ axis is the activation of the other one $\\rightarrow$ MSE incorrectly penalizes outputs which are perfectly valid for prediction\n",
    "    <img width=60% src=\"images/5-2.png\">\n",
    "- If a network should compute log-probabilities, it may have a `torch.nn.LogSoftmax` final layer, and be trained with `torch.nn.NLLLoss`\n",
    "- Soft-max mapping:\n",
    "    <img width=40% src=\"images/5-3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0612e-09,  2.0612e-09,  1.0000e+00,  3.0590e-07],\n",
      "        [ 8.7005e-01,  4.3317e-02,  4.3317e-02,  4.3317e-02],\n",
      "        [ 3.2059e-02,  8.7144e-02,  2.3688e-01,  6.4391e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tlvu2697/miniconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Example of Soft-max layer\n",
    "\"\"\"\n",
    "x = Variable(Tensor([[-10, -10, 10, -5],\n",
    "                     [  3,   0,  0,  0],\n",
    "                     [  1,   2,  3,  4]]))\n",
    "f = torch.nn.Softmax()\n",
    "print(f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Other Loss functions provided by PyTorch:\n",
    "    1. `torch.nn.MSELoss`\n",
    "    2. `torch.nn.CrossEntropyLoss`\n",
    "    3. `torch.nn.NLLLoss`\n",
    "    4. `torch.nn.L1Loss`\n",
    "    5. `torch.nn.NLLLoss2d`\n",
    "    6. `torch.nn.MultiMarginLoss`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stochastic gradient descent\n",
    "- Disadvantage of traditional Gradient descent:\n",
    "    - Takes time to compute\n",
    "    - Computation redundancy\n",
    "    - Bad performence due to computing repeatly $l_{n}$\n",
    "    - Difficult to choose efficient step size\n",
    "- **Stochastic gradient descent**\n",
    "    $$w_{t+1} = w_{t} - \\eta \\nabla l_{\\,n(t)}(w_{t})$$\n",
    "$\\rightarrow$ Does not benefit from the speed-up of batch-processing\n",
    "- **Mini-batch Stochastic gradient descent**\n",
    "    - Standard procedure for deep learning\n",
    "        $$w_{t+1} = w_{t} - \\eta \\sum^{B}_{b = 1} \\nabla l_{\\,n(t,\\,b)}(w_{t})$$\n",
    "    - Help evade local minima\n",
    "    - Performance\n",
    "        <img width=60% src=\"images/5-4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Momemtum & Moment estimation\n",
    "- Vanilla mini-batch Stochastic gradient descent (SGD) consist of 2 parts\n",
    "    1. $w_{t+1} = w_{t} - \\eta\\, g_{t}$\n",
    "    2. where $g_{t} = \\sum^{B}_{b=1}\\nabla\\, l_{n(t,\\,b)}(w_{t})$ is the gradient summed over a mini-btach\n",
    "- Improvements:\n",
    "    1. Momentum, to add inertia in the choise of the step direction\n",
    "        $$u_{t} = \\gamma\\, u_{t-1} + \\eta\\,g_{t}$$\n",
    "        $$w_{t+1} = w_{t} - u_{t}$$\n",
    "        - With $\\gamma = 0$, this is the same as vanilla SGD\n",
    "            <img width=40% src=\"images/5-5.png\">\n",
    "        - With $\\gamma > 0$:\n",
    "            - It can \"go through\" local barriers\n",
    "            - It accelerates if the gradient does not change much\n",
    "            - It dampens oscillations in narrow valleys\n",
    "            <img width=40% src=\"images/5-6.png\">\n",
    "    2. Adam algorithm\n",
    "\n",
    "|||\n",
    "|---|---|\n",
    "|<img src=\"images/5-7.png\">|<img src=\"images/5-8.png\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `torch.optim`\n",
    "- Implementing the standard SGD with `torch.optim`:\n",
    "    - Normal Vanilla SGD\n",
    "    ```python\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = eta)\n",
    "    ...\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ```\n",
    "    - Vanilla SGD + Adam algorithm\n",
    "    ```python\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "    ...\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ```\n",
    "- Other optimizations:\n",
    "    - `torch.optim.SGD` (momentum, Nesterov's algorithm)\n",
    "    - `torch.optim.Adam`\n",
    "    - `torch.optim.Adadelta`\n",
    "    - `torch.optim.Adagrad`\n",
    "    - `torch.optim.RMSprop`\n",
    "    - `torch.optim.LBFGS`\n",
    "- **The learning rate may have to be different if the functional was not properly scaled**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. An example putting all this together\n",
    "- Tools to define a deep network:\n",
    "    - fully connected layer (torch.nn.Linear)\n",
    "    - convolutional layer (torch.nn.Conv2d)\n",
    "    - pooling layer\n",
    "    - ReLU\n",
    "- Tools to optimize a deep network:\n",
    "    - Loss\n",
    "    - Back-propagation\n",
    "    - Stochastic gradient descent\n",
    "- PyTorch initialize paramters as normalize weights according to the layer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model found\n",
      "Loading model: Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Example putting all things together\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PATH = 'model/5-init-optim-ex5.pth.tar'\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_model():\n",
    "    train_set = datasets.MNIST('./data/mnist/', train = True, download = True)\n",
    "    train_input = Variable(train_set.train_data.view(-1, 1, 28, 28).float())\n",
    "    train_target = Variable(train_set.train_labels)\n",
    "\n",
    "    model, criterion = Net(), nn.CrossEntropyLoss()\n",
    "    \n",
    "    if cuda.is_available():\n",
    "        print('(Cuda is available) ', end='')\n",
    "        model.cuda()\n",
    "        criterion.cuda()\n",
    "        train_input, train_target = train_input.cuda(), train_target.cuda()\n",
    "\n",
    "    ### Scaling Data\n",
    "    mu, std = train_input.data.mean(), train_input.data.std()\n",
    "    train_input.data.sub_(mu).div_(std)\n",
    "\n",
    "    ### Constants\n",
    "    lr, nb_epochs, batch_size = 1e-1, 10, 100\n",
    "    optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "    for k in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), batch_size):\n",
    "            output = model(train_input.narrow(0, b, batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print('Done')\n",
    "    \n",
    "    torch.save(model, PATH)\n",
    "    print('Save model: Done')\n",
    "    \n",
    "    return model\n",
    "\n",
    "if os.path.exists(PATH):\n",
    "    print('Pretrained model found')\n",
    "    model = torch.load(PATH)\n",
    "    print('Loading model: Done')\n",
    "else:\n",
    "    print('Pretrained model not found')\n",
    "    print('Training: ', end='')\n",
    "    model = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. $L_{2}$ and $L_{1}$ penalties\n",
    "- $L_{2}$ regularization:\n",
    "    $$\\lambda\\, |w|\\,^{2}_{2}$$\n",
    "- $L_{1}$ regularization:\n",
    "    $$\\lambda\\, |w|\\,_{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Weight initialization\n",
    "- Rely on controlling\n",
    "$$\\vee\\, (\\frac{\\delta\\,l}{\\delta\\,w^{(l)}_{i,\\,j}}) \\,\\text{ and }\\, \\vee\\, (\\frac{\\delta\\,l}{\\delta\\,b^{(l)}_{i}})$$\n",
    "so that\n",
    "    - the gradient does not vanish\n",
    "    - weights evolve at the same rate across layers during training, and no layer reaches a saturation behavior before others\n",
    "- Types of initialization:\n",
    "    1. Controlling the Variance of activations\n",
    "        $$\\vee\\,(w^{(l)}) = \\frac{1}{N_{l - 1}}$$\n",
    "    ```python\n",
    "        def reset_parameters(self):\n",
    "            stdv = 1. / math.sqrt(selft.weight.size(1))\n",
    "            self.weight.data.uniform_(-stdv, stdv)\n",
    "            if self.bias is not None:\n",
    "                self.bias.data.uniform_(-stdv, stdv)\n",
    "    ```\n",
    "    2. Controllling the Variance of the gradient with activations\n",
    "        $$\\vee\\,(w^{(l)}) = \\frac{1}{N_{l}}$$\n",
    "    3. Xavier Initialization\n",
    "        $$\\vee\\,(w^{(l)}) = \\frac{2}{N_{l - 1} + N_{l}}$$\n",
    "    ```python\n",
    "        def xavier_normal(tensor, gain=1):\n",
    "            if isinstance(tensor, Variable):\n",
    "                xavier_normal(tensor.data, gain=gain)\n",
    "                return tensor\n",
    "            \n",
    "            fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n",
    "            std = gain * math.sqrt(2.0 / (fan_in + fan_out))\n",
    "            return tensor.normal_(0, std)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Data normalization\n",
    "- Normal method\n",
    "```python\n",
    "mu, std = train_input.mean(), train_input.std()\n",
    "train_input.sub_(mu).div_(std)\n",
    "test_input.sub_(mu).div_(std)\n",
    "```\n",
    "- Component-wise method\n",
    "```python\n",
    "mu, std = train_input.mean(0), train_input.std(0)\n",
    "train_input.sub_(mu).div_(std)\n",
    "test_input.sub_(mu).div_(std)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Choice of architecture and step size\n",
    "- Choosing the network structure is a difficult exercise. Strategy:\n",
    "    - Re-use something \"well known, that works\"\n",
    "    - Split feature extraction / inference\n",
    "    - Modulate the capacity until if overfits a small subset, but does not overfit / underfit the full set\n",
    "    - Capacity increases with more layers, more channels, larger receptive fields or more units\n",
    "    - Regularization to reduce the capativy or induce sparsity\n",
    "    - Identify common paths for siamese-lise\n",
    "    - Idenfify what path(s) or sub-oarts need more/less capacity\n",
    "    - Use prior knowledge about the \"scale of meaningful context\" to size filters / combinations of filters\n",
    "    - Grid-search all the variations that come to mind\n",
    "- Requirement for learning rate\n",
    "    - Reduce loss quickly -> large lr\n",
    "    - Not be trapped in a bad minimum -> large lr\n",
    "    - Not bounce around in narrow valleys -> small lr\n",
    "    - Not oscillate around a minimun -> small lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Writing a `torch.autograd.Function`\n",
    "- Need to implement 2 static methods\n",
    "    1. `forward()`\n",
    "    2. `backward()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
