{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Going deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Initialization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Benefits and challenges of greater depth\n",
    "- Trending: deeper architectures -> improve performance\n",
    "- $\\forall f,\\,\\kappa\\,(\\sigma\\,(\\,f)) \\leq 2\\,\\kappa\\,(\\,f)$\n",
    "- $\\forall (\\,f, g),\\,\\kappa\\,(\\,f + g) \\leq \\kappa\\,(\\,f) + \\kappa\\,(\\,g)$\n",
    "- For any ReLU MLP: $\\kappa\\,(\\,y) \\leq 2^{D} \\prod_{d = 1}^{D}\\,W_{d}$\n",
    "- Have to ensure:\n",
    "    1. The gradient does not \"vanish\"\n",
    "    2. Gradient amplitude is homogeneous so that all parts of the network train at the same rate\n",
    "    3. The gradient does not vary too unpredictably when the weights change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Rectifiers\n",
    "- $ReLU$ is much better than $tanh$ function because:\n",
    "    1. The derivative of $ReLU$ itself not vanishing\n",
    "    2. Under experiment, a 4-layer CNN with $ReLU$ reaches a 25% training error rate on CIFAR-10 6 times faster than an equivalent network with $tanh$ neurons\n",
    "- Variants of $ReLU$:\n",
    "    1. $Leaky-ReLU$:\n",
    "    $$x \\mapsto max(ax, x) \\text{ with } 0 \\leq a < 1$$\n",
    "        - Parameter $a$ can be either fixed or optimized during training\n",
    "    2. $\"maxout\"$ layer - Goodfellow (2013)\n",
    "    3. Concatenated Rectified Linear Unit (CReLU) - Shang (2016)\n",
    "    $$R \\mapsto R^{2}$$\n",
    "    $$x \\mapsto (max(0,\\,x),\\,max(0,\\,-x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dropout\n",
    "- Removing units at random during the forward pass on each sample, and putting them all back during test\n",
    "<img width=80% src=\"images/6-1.png\">\n",
    "- Purpose:\n",
    "    1. Increase independence between units\n",
    "    2. Distribyte the representation\n",
    "    3. Improve performance\n",
    "\n",
    "> \"Units may change in a way that they fix up the mistakes of the other units\" (Srivastava, 2014) $\\rightarrow$ That's the reason for the first and the third purpose\n",
    "\n",
    "- $p$: probability for units to be dropped (default = 0.5)\n",
    "- Dropout is not implemented by actually swotching off unit,, but equivalently as a module that drops activations at random on each sample\n",
    "- `torch.nn.DropOut` which is a `torch.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])\n",
      "tensor([[ 0.7559,  0.7559,  0.7559,  0.7559,  0.7559,  0.0000,  0.0000,\n",
      "          0.7559,  0.7559],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.1547,  1.1547,  0.0000,  1.1547,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.8944,  0.0000,  0.0000,  0.8944,  0.8944,\n",
      "          0.8944,  0.8944]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Example of 'torch.nn.DropOut' function\n",
    "\"\"\"\n",
    "\n",
    "x = Variable(Tensor(3, 9).fill_(1.0), requires_grad = True)\n",
    "print(x.data)\n",
    "\n",
    "# Dropout = Forward + Dropping\n",
    "dropout = nn.Dropout(p = 0.5)\n",
    "y = dropout(x) # == .forward()\n",
    "\n",
    "l = y.norm(2, 1).sum()\n",
    "l.backward()\n",
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add DropOut layer to network:\n",
    "    - Original:\n",
    "    ```python\n",
    "    model = nn.Sequential(nn.Linear(10, 100), nn.ReLU(),\n",
    "                            nn.Linear(100, 50), nn.ReLU(),\n",
    "                            nn.Linear(50, 2));\n",
    "    ```\n",
    "    ---\n",
    "    - Adding dropout layers:\n",
    "    ```python\n",
    "    model = nn.Sequential(nn.Linear(10, 100), nn.ReLU(),\n",
    "                            nn.Dropout()\n",
    "                            nn.Linear(100, 50), nn.ReLU(),\n",
    "                            nn.Dropout()\n",
    "                            nn.Linear(50, 2));\n",
    "    ```\n",
    "- A model using dropout has to be set in \"train\" or \"test\" mode\n",
    "- **Variant of DropOut**:\n",
    "    1. **SpatialDropout** - dropping channels instead of individual units\n",
    "    2. **DropConnect** - dropping connection instead of individual units. Can't be implemented as a separate layer and it's computationally intensive\n",
    "- Performance comparison\n",
    "<img width=60% src=\"images/6-2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 2.,  2.],\n",
      "          [ 2.,  2.]],\n",
      "\n",
      "         [[ 0.,  0.],\n",
      "          [ 0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.],\n",
      "          [ 0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 2.,  2.],\n",
      "          [ 2.,  2.]],\n",
      "\n",
      "         [[ 0.,  0.],\n",
      "          [ 0.,  0.]],\n",
      "\n",
      "         [[ 0.,  0.],\n",
      "          [ 0.,  0.]]]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Example of SpatialDropout - 'torch.nn.Dropout2d\n",
    "\"\"\"\n",
    "x = Variable(Tensor(2, 3, 2, 2).fill_(1.0))\n",
    "dropout2d = nn.Dropout2d()\n",
    "\n",
    "print(dropout2d(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Activation normalization\n",
    "- Explicitly forcing the activation statistics during the forward pass by re-normalizing them\n",
    "- **Batch normalization**:\n",
    "    - Can be done anywhere in a deep architecture\n",
    "    - Force the activation's first and second order moments $\\rightarrow$ the following layers don't need to adapt to their drift\n",
    "    - Shift and rescale according to the mean and variance estimated on the batch during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.0133,  24.9739,   2.7379])\n",
      "tensor([ 1.9520,  5.0703,  9.9830])\n",
      "tensor([ 2.0000,  4.0000,  8.0000])\n",
      "tensor([ 1.0005,  2.0010,  3.0015])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Example of 'torch.BatchNorm1d'\n",
    "\"\"\"\n",
    "\n",
    "x = Tensor(1000, 3).normal_()\n",
    "x = x * Tensor([2, 5, 10]) + Tensor([-10, 25, 3])\n",
    "x = Variable(x)\n",
    "\n",
    "print(x.data.mean(0))\n",
    "print(x.data.std(0))\n",
    "\n",
    "bn = nn.BatchNorm1d(3)\n",
    "bn.bias.data = Tensor([2, 4, 8])\n",
    "bn.weight.data = Tensor([1, 2, 3])\n",
    "y = bn(x)\n",
    "\n",
    "print(y.data.mean(0))\n",
    "print(y.data.std(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Residual network\n",
    "- A residual network uses a building block with a pass-through identity mapping\n",
    "    <img width=80% src=\"images/6-3.png\">\n",
    "    <img width=80% src=\"images/6-4.png\">\n",
    "- This structure allow the parameters to be optimized to learn a residual (the difference between the value before the block and the one needed after)\n",
    "- Purpose:\n",
    "    1. Reduce the activation map size by a factor 2:\n",
    "        <img width=80% src=\"images/6-5.png\">\n",
    "    2. Increase the number of channels: from $C$ to $C'$\n",
    "        - Pad the original value with $C' - C' zeros$\n",
    "        - Use $C'$ convolutions with a $1 \\times 1 \\times C$ filter\n",
    "- Residual networks are **fully convolutional**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Smart initialization\n",
    "- **Layer-Sequential Unit-Variance** initialization:\n",
    "    1. Initialize the weights of all layers with orthonormal matrices\n",
    "    2. Re-scale layers one after another in a forward direction, so that the empirical activation variance is $1.0$\n",
    "- Suggestion: combine CReLU with a **Looks Linear initialization**\n",
    "    <img width=40% src=\"images/6-6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Summarize\n",
    "- Techniques enable the training of very deep architectures:\n",
    "    1. **Rectifiers** to prevent  the gradient from vanishing during the backward pass\n",
    "    2. **Drop-out** to force a distributed representation\n",
    "    3. **Batch normalization** to dynamically maintain the statistics of activations\n",
    "    4. **Identity pass-through** to keep a structured gradient and distribute representation\n",
    "    5. **Smart initialization** to put the gradient in a good regime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
