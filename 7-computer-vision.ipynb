{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Networks for computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Initialization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Task and data-set\n",
    "- Tasks:\n",
    "    - Classification\n",
    "    - Object detection\n",
    "    - Semantic or instance segmentation\n",
    "    - Other (tracking in videos, camera pose estimation, body pose estimation, 3d reconstruction, denoising, super-resolution, auto-captioning, synthesis,...)\n",
    "- Small scale classification data-sets:\n",
    "    - MNIST and Fashion-MNIST: 10 classes, 50000 train iamges, 1000 test images, 28 $\\times$ 28 grayscale\n",
    "    - CIFAR10 (10 classes) and CIFAR100 (5 $\\times$ 20 super classes): 50000 train images, 10000 test images, 32 $\\times$ 32 RGB\n",
    "    - PASCAL VOC 2012: 20 classes, 11530 training + validation images\n",
    "    - ImageNet (image-net.org): 14197122 images\n",
    "    - ImageNet Large Scale Visual Recognition Challenge 2012: 1000 classes, 1200000 training images, 50000 validation images\n",
    "    - Cityscapes (cityscapes-dataset.com): 30 classes, 5000 images with fine annotations, 20000 images with coarse annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Task and performance measure\n",
    "- Image classification:\n",
    "    1. Task: predicting its class\n",
    "    2. Performance measure:\n",
    "        - The **error rate** $P(\\,f(X) \\neq y)$ or **accuracy** $P(\\,f(X) = y)$\n",
    "        - The **balanced error rate** (BER) $\\frac{1}{C} \\sum^{C}_{y = 1} P(\\,f(X) \\neq y\\,|\\,Y = y)$\n",
    "        - In two-class case, define **True Positive** and **False Positive** (idea algorithms have TP $\\approx$ 1 and FP $\\approx$ 0) $\\rightarrow$ Area under **TP** vs **FP** curve is **Receiver operating characteristic** (ROC)\n",
    "            <img width=40% src=\"images/7-1.png\">\n",
    "        - Another curve is **Precision** vs **Recall** $\\rightarrow$ Area under **Precision** vs **Recall** curve is **Average precicion**\n",
    "            > True Positive: $P(\\,f(X) = 1\\,|\\,Y = 1)$  \n",
    "            > False Positive: $P(\\,f(X) = 1\\,|\\,Y = 1)$  \n",
    "            > Presision: $P(\\,f(X) = 1\\,|\\,Y = 1)$  \n",
    "            > True Positive: $P(\\,f(X) = 1\\,|\\,Y = 1)$  \n",
    "\n",
    "- Object detection:\n",
    "    1. Task: predicting classes and locations of targets in an image, output is a series of bounding boxes, each with a class label\n",
    "    2. Performance measure: consider predicted bounding box $B'$ and annotated bounding box $B$, we always want the **Intersetion over Union (IoU)** is large enough\n",
    "        <img width=40% src=\"images/7-2.png\">\n",
    "- Image segmentation:\n",
    "    1. Task: labeling individual pixels with the class of the object or the instance it belongs to\n",
    "    2. Performance measure: (classification) **Segmentation accuracy (SA)**\n",
    "        $$SA = \\frac{n}{n + e}$$\n",
    "    > $n$: number of pixels of the right class predicted  \n",
    "    > $e$: number of pixels erroneously labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Image classification, standard convnets\n",
    "- The most standard networks for image classification:\n",
    "    1. LeNet\n",
    "        - LeNet5: 10 classes, input $1 \\times 28 \\times 28$\n",
    "        <img width=60% src=\"images/7-3.png\">\n",
    "\n",
    "    2. AlexNet\n",
    "        - 1000 classes, input $3 \\times 224 \\times 224$\n",
    "        - Use **Data augmentation** during training to reduce over-fitting\n",
    "        <img width=60% src=\"images/7-4.png\">\n",
    "\n",
    "    3. VGGNet\n",
    "        - 1000 classes, input $3 \\times 224 \\times 224$\n",
    "        - 16 convolutional layers + 3 fully connected layers\n",
    "        <img width=60% src=\"images/7-5.png\">\n",
    "        <img width=60% src=\"images/7-6.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 244, 244])\n",
      "#0 (17.98) trolleybus, trolley coach, trackless trolley\n",
      "#1 (16.24) minibus\n",
      "#2 (15.64) passenger car, coach, carriage\n",
      "#3 (14.38) fire engine, fire truck\n",
      "#4 (14.30) streetcar, tram, tramcar, trolley, trolley car\n",
      "#5 (13.73) electric locomotive\n",
      "#6 (12.39) recreational vehicle, RV, R.V.\n",
      "#7 (11.99) harvester, reaper\n",
      "#8 (10.84) trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi\n",
      "#9 (10.83) minivan\n",
      "#10 (10.29) moving van\n",
      "#11 (10.15) tow truck, tow car, wrecker\n",
      "#12 (9.96) amphibian, amphibious vehicle\n",
      "#13 (9.95) ambulance\n",
      "#14 (9.77) school bus\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Example of Image classification\n",
    "\"\"\"\n",
    "\n",
    "def image_squaring(raw_image):\n",
    "    raw_image_size = raw_image.size\n",
    "    if raw_image_size[0] == raw_image_size[1]:\n",
    "        return raw_image\n",
    "    new_image_size = (max(raw_image_size), max(raw_image_size))\n",
    "    new_image = PIL.Image.new(\"RGB\", new_image_size)\n",
    "    new_image.paste(raw_image, (int((new_image_size[0] - raw_image_size[0])/2),\n",
    "                                  int((new_image_size[1] - raw_image_size[1])/2)))\n",
    "    return new_image\n",
    "\n",
    "def image_resizing(raw_image, size):\n",
    "    raw_image.thumbnail(size, PIL.Image.ANTIALIAS)\n",
    "    return raw_image\n",
    "\n",
    "def image_preprocessing(raw_image, size):\n",
    "    return image_resizing(image_squaring(raw_image), size)\n",
    "\n",
    "# Load and nomalize the image\n",
    "raw_image = PIL.Image.open('data/images/image4.jpg')\n",
    "img = torchvision.transforms.ToTensor()(image_preprocessing(raw_image, (244, 244)))\n",
    "img = img.view(1, img.size(0), img.size(1), img.size(2))\n",
    "img = 0.5 + 0.5 + (img - img.mean()) / img.std()\n",
    "print(img.size())\n",
    "\n",
    "# Load an already trained network and compute its prediction\n",
    "alexnet = torchvision.models.alexnet(pretrained = True)\n",
    "alexnet.eval()\n",
    "\n",
    "output = alexnet(Variable(img))\n",
    "\n",
    "# Print the classes\n",
    "scores, indexes = output.data.view(-1).sort(descending = True)\n",
    "class_names = eval(open('model/imagenet1000_clsid_to_human.txt', 'r').read())\n",
    "\n",
    "for k in range(15):\n",
    "    print('#{:d} ({:.02f}) {:s}'.format(k, scores[k], class_names[int(indexes[k])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fully convolutional networks\n",
    "- Transform a series of layers from a standard convnets to fully convolutional (convolutionize) \n",
    "\n",
    "|||\n",
    "|---|---|\n",
    "|<img src=\"images/7-7.png\">|<img src=\"images/7-8.png\">|\n",
    "\n",
    "- Pratical consequence:\n",
    "    - Re-use classification networks for **dense prediction** without re-training\n",
    "    - Blur the conceptual boundary between \"features\" and \"classifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Image classification\n",
    "#### a. Network in network\n",
    "- Re-interpret a convolution filter as a one-layer perceptron and extened it with an \"MLP convolution\" to omprove the capacity vs parameter ratio\n",
    "    <img width=60% src=\"images/7-9.png\">\n",
    "    <img width=80% src=\"images/7-10.png\">\n",
    "- \"Auxiliary classifiers\" help the propagation of the gradient in the early layers $\\rightarrow$ increase performance by the idea that early layers already encode informative and invariant features $\\rightarrow$ GoogLeNet has 12 times less parameter than AlexNet but more accurate in ILSVRC14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Residual networks\n",
    "<img width=40% src=\"images/7-11.png\">\n",
    "\n",
    "#### c. Summary\n",
    "- Standard ones are **extensions of LeNet5**\n",
    "- Everybody loves **ReLU**\n",
    "- State-of-the-art networks have **100s of channels** and **10s of layers**\n",
    "- Networks sould be **fully convolutional**\n",
    "- **Pass-through connections** allow deeper \"residual\" nets\n",
    "- **Bottelneck local structures** and **Aggregated pathways** reduce the number of parameters\n",
    "\n",
    "<img width=80% src=\"images/7-12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Object detection\n",
    "- Simplest strategy is to classify local regions, at multiple scales and locations (kinda brute-force @\\_@!) $\\rightarrow$ cost increases with prediction accuracy\n",
    "- The above strategy is mitigated by Sermanet by adding a regression part to predict object's bounding box\n",
    "<img width=30% src=\"images/7-13.png\">\n",
    "- Example of bounding boxes produced by the regression network mentioned above\n",
    "<img width=60% src=\"images/7-14.png\">\n",
    "    $\\rightarrow$ Combining multiple boxes is done with an *ad hoc* greedy algorithm\n",
    "- AlexNet approach: relying on **region proposals**\n",
    "    - Generate thousands of proposal bounding boxes with a non-CNN \"objectness\" approach\n",
    "    - Feed to an AlexNet-like network sub-images cropped and warp from the input image to detect\n",
    "    $\\rightarrow$ Suffer from the cost of the region proposal computation, which is non-convolutional and non-GPUified\n",
    "- Ren with \"Faster R-CNN\" improve AlexNet by replacing the region proposal algorithm with a convolutional processing similar to Overfeat\n",
    "- Most famous algorithm is **\"You Only Look Once\"** (YOLO, Redmon). Mechanism:\n",
    "    <img width=80% src=\"images/7-15.png\">\n",
    "    <img width=60% src=\"images/7-16.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Semantic segmentation\n",
    "<img width=60% src=\"images/7-17.png\">\n",
    "- Historical approach: define a measure of similarity between pixels, and to cluster gourps of similar pixels (poorly performance)\n",
    "- Deep-learning approach: re-casts semantic segmentation as pixel classification, and re-uses networks trained for image classification by making them fully convolutional\n",
    "    <img width=60% src=\"images/7-18.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. `torch.utils.data.DataLoader`\n",
    "- Large sets do not fit in memory, and samples have to be constanly loaded during training $\\rightarrow$ `torch.utils.data.DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'num_workders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ea250df73b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mnum_workders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpin_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_workders'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Example of 'torch.utils.data.DataLoader'\n",
    "\"\"\"\n",
    "train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(28, padding = 3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = (33.32, ), std = (78.56, ))\n",
    "        ]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST(root = './data', train = True, download = True,\n",
    "                  transform = train_transforms),\n",
    "    batch_size = 100,\n",
    "    num_workders = 4,\n",
    "    shuffle = True,\n",
    "    pin_memory = torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given this 'train_loader', we can now rewrite our training procedure with a loop over the mini-batches\n",
    "    - Before:\n",
    "\n",
    "    ```python\n",
    "    if torch.cuda.is_available():\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "    input, target = Varible(input), Variable(target)\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    ```\n",
    "    ---  \n",
    "\n",
    "    - After:\n",
    "\n",
    "    ```python\n",
    "    for e in range(nb_epochs):\n",
    "        for input, target in iter(train_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            input, target = Varible(input), Variable(target)\n",
    "\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
